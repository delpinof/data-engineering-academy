{"paragraphs":[{"text":"%md\n# Iterations and matrix multiplication\n\nWelcome to the notebook with the asasignment for the second session. You’re well on your way to obtain the Wizeline Certification for Big Data Engineering with Spark!\n\nIf you have any feedback about our courses, email us at academy@wizeline.com or use the Academy Slack channel.","dateUpdated":"2018-08-18T01:31:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1534545085445_-1270863205","id":"20180815-184319_767984790","dateCreated":"2018-08-17T22:31:25+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:196"},{"text":"%md\n## Matrix Multiplication Assignment\n\nIn this exercise, you will be implementing a Matrix multiplication on a larger dataset than the one you’ve used so far. \n\nYou will be challenged to work with two Datasets, transform them into `CoordinateMatrix` objects, and ultimately convert each to a `BlockMatrix`.\n","dateUpdated":"2018-08-18T01:31:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Matrix Multiplication Assignment</h2>\n<p>In this exercise, you will be implementing a Matrix multiplication on a larger dataset than the one you’ve used so far. </p>\n<p>You will be challenged to work with two Datasets, transform them into <code>CoordinateMatrix</code> objects, and ultimately convert each to a <code>BlockMatrix</code>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1534545085447_-1270093708","id":"20180816-044039_285868597","dateCreated":"2018-08-17T22:31:25+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:197"},{"text":"%md\n\n```\nFile Format:\nEach line in the text file is a row in the matrix. They are comma-separated values. The first value is the row index starting at one, the remaining are the Matrix values. \n```\n\nFirst, import the libraries to use during the session with the following command:\n\n\n","dateUpdated":"2018-08-18T01:31:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<pre><code>File Format:\nEach line in the text file is a row in the matrix. They are comma-separated values. The first value is the row index starting at one, the remaining are the Matrix values. \n</code></pre>\n<p>First, import the libraries to use during the session with the following command:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1534545085448_-1272017452","id":"20180816-050807_2103207409","dateCreated":"2018-08-17T22:31:25+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:198"},{"text":"import org.apache.spark.sql.Dataset\nimport org.apache.spark.mllib.linalg.distributed.{BlockMatrix, CoordinateMatrix, MatrixEntry}","user":"anonymous","dateUpdated":"2018-08-18T01:31:22+0000","config":{"tableHide":false,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.Dataset\nimport org.apache.spark.mllib.linalg.distributed.{BlockMatrix, CoordinateMatrix, MatrixEntry}\n"}]},"apps":[],"jobName":"paragraph_1534545085449_-1272402201","id":"20180816-051038_545285622","dateCreated":"2018-08-17T22:31:25+0000","dateStarted":"2018-08-18T01:15:15+0000","dateFinished":"2018-08-18T01:15:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:199"},{"text":"%md\nNext use `spark.read.textFile` to read the two matrices (matrix1.txt and matrix2.txt) on the bucket URI: \n`gs://de-training-input/matrices/matrix*.txt`.\n\nBecause there are two matrices, make a read call for each and store them in separate variables m1s and m2s. \n\nDon’t forget to map a split by “,” while reading the files.\n","dateUpdated":"2018-08-18T01:31:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Next use <code>spark.read.textFile</code> to read the two matrices (matrix1.txt and matrix2.txt) on the bucket URI:<br/><code>gs://de-training-input/matrices/matrix*.txt</code>.</p>\n<p>Because there are two matrices, make a read call for each and store them in separate variables m1s and m2s. </p>\n<p>Don’t forget to map a split by “,” while reading the files.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1534545085449_-1272402201","id":"20180816-051039_1898536718","dateCreated":"2018-08-17T22:31:25+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:200"},{"text":"val m1s = sc.textFile(\"gs://de-training-input/matrices/matrix1.txt\").toDS\nval m2s = sc.textFile(\"gs://de-training-input/matrices/matrix2.txt\").toDS\n\nvar row1 = m1s.take(1)","user":"anonymous","dateUpdated":"2018-08-18T01:31:04+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"},"editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"m1s: org.apache.spark.sql.Dataset[String] = [value: string]\nm2s: org.apache.spark.sql.Dataset[String] = [value: string]\nrow1: Array[String] = Array(0,0.8604010638563133,0.7177783965053582,0.9248373557452342,0.5386150238914319,0.4796233652545463,0.1246384286708101,0.44848594807946696,0.15333652606943904,0.9809053413536545,0.5877792283562825,0.7743011662467651,0.6517035361922311,0.3849749586529424,0.9348783444735791,0.5781369977671532,0.2516417298632936,0.22584475988758412,0.7777588312945031,0.7739523609472254,0.19531270771723264,0.45994071815635496,0.028498822947872093,0.13311882770852212,0.8918641175641797,0.08313321227142512,0.9742500952081414,0.8635988717371847,0.11463192484701579,0.8621944311036688,0.2989800293243826,0.7396009484184309,0.2196220627747827,0.775683778206351,0.7303899203991329,0.3264198938739252,0.03276269180715097,0.7289888323379159,0.31155192620486516,0.08052253129199971,0.265332906026..."}]},"apps":[],"jobName":"paragraph_1534545085450_-1271247954","id":"20180816-070420_1766932377","dateCreated":"2018-08-17T22:31:25+0000","dateStarted":"2018-08-18T01:15:20+0000","dateFinished":"2018-08-18T01:15:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:201"},{"text":"%md\nNow that we have loaded the data into Spark, we need to transform them into the `MatrixEntry` class that was imported earlier. To do so use a `flatMap` to create the required objects. \n\nYour mapper will need to iterate over each item in the row to correctly yield the parameters required by `MatrixEntry`. If you use `.view.zipWithIndex` on each line, you get an iterator that will contain (`value, list_index`).\n\n\nHINT: How do you remove the first value from each line from the `.zipWithIndex` method call? Does `head` ring a bell?\n","dateUpdated":"2018-08-18T01:31:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Now that we have loaded the data into Spark, we need to transform them into the <code>MatrixEntry</code> class that was imported earlier. To do so use a <code>flatMap</code> to create the required objects. </p>\n<p>Your mapper will need to iterate over each item in the row to correctly yield the parameters required by <code>MatrixEntry</code>. If you use <code>.view.zipWithIndex</code> on each line, you get an iterator that will contain (<code>value, list_index</code>).</p>\n<p>HINT: How do you remove the first value from each line from the <code>.zipWithIndex</code> method call? Does <code>head</code> ring a bell?</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1534545085451_-1271632703","id":"20180816-203508_664801339","dateCreated":"2018-08-17T22:31:25+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:202"},{"text":"def create_mEntries(row:String) : MatrixEntry = {\n    var c = row.split(\",\")\n    for(i <- 1 until c.size) yield \n        org.apache.spark.mllib.linalg.distributed.MatrixEntry(c(0).toLong,i-1,c(i).toDouble)\n}","user":"anonymous","dateUpdated":"2018-08-18T01:31:04+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"<console>:33: error: type mismatch;\n found   : scala.collection.immutable.IndexedSeq[org.apache.spark.mllib.linalg.distributed.MatrixEntry]\n required: org.apache.spark.mllib.linalg.distributed.MatrixEntry\n           for(i <- 1 until c.size) yield\n                 ^\n"}]},"apps":[],"jobName":"paragraph_1534550226711_213233130","id":"20180817-235706_744189084","dateCreated":"2018-08-17T23:57:06+0000","dateStarted":"2018-08-18T01:15:25+0000","dateFinished":"2018-08-18T01:15:25+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:203"},{"text":"\nval mEntries1 = m1s.flatMap(row => {\n    var c = row.split(\",\")\n    for(i <- 1 until c.size) yield \n        MatrixEntry(c(0).toLong,i-1,c(i).toDouble)\n})\n\nval mEntries2 = m2s.flatMap(row => {\n    var c = row.split(\",\")\n    for(i <- 1 until c.size) yield \n        MatrixEntry(c(0).toLong,i-1,c(i).toDouble)\n})","user":"anonymous","dateUpdated":"2018-08-18T01:31:04+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"mEntries1: org.apache.spark.sql.Dataset[org.apache.spark.mllib.linalg.distributed.MatrixEntry] = [i: bigint, j: bigint ... 1 more field]\nmEntries2: org.apache.spark.sql.Dataset[org.apache.spark.mllib.linalg.distributed.MatrixEntry] = [i: bigint, j: bigint ... 1 more field]\n"}]},"apps":[],"jobName":"paragraph_1534554330227_-334428784","id":"20180818-010530_113368376","dateCreated":"2018-08-18T01:05:30+0000","dateStarted":"2018-08-18T01:15:28+0000","dateFinished":"2018-08-18T01:15:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:204"},{"text":"%md\nNow that we have converted all of our lines into `MatrixEntry` objects we need to put them all inside a `BlockMatrix`. Define a function called `create_blockMatrix` that receives one of the previous objects and uses a `CoordinateMatrix` to output a `BlockMatrix`.\n","dateUpdated":"2018-08-18T01:31:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":false,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Now that we have converted all of our lines into <code>MatrixEntry</code> objects we need to put them all inside a <code>BlockMatrix</code>. Define a function called <code>create_blockMatrix</code> that receives one of the previous objects and uses a <code>CoordinateMatrix</code> to output a <code>BlockMatrix</code>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1534545085452_-1273556448","id":"20180816-052746_2109723812","dateCreated":"2018-08-17T22:31:25+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:205"},{"text":"def create_blockMatrix(matrixEntries:Dataset[MatrixEntry]) : BlockMatrix = new CoordinateMatrix(matrixEntries.rdd).toBlockMatrix()","user":"anonymous","dateUpdated":"2018-08-18T01:31:04+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"},"editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"create_blockMatrix: (matrixEntries: org.apache.spark.sql.Dataset[org.apache.spark.mllib.linalg.distributed.MatrixEntry])org.apache.spark.mllib.linalg.distributed.BlockMatrix\n"}]},"apps":[],"jobName":"paragraph_1534545085453_-1273941197","id":"20180816-051640_149771352","dateCreated":"2018-08-17T22:31:25+0000","dateStarted":"2018-08-18T01:17:45+0000","dateFinished":"2018-08-18T01:17:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:206"},{"text":"%md\nNow, we convert each `Dataset` into a `BlockMatrix`:","dateUpdated":"2018-08-18T01:31:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Now, we convert each <code>Dataset</code> into a <code>BlockMatrix</code>:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1534545085453_-1273941197","id":"20180816-051434_624669525","dateCreated":"2018-08-17T22:31:25+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:207"},{"text":"val blockM1 = create_blockMatrix(mEntries1)\nval blockM2 = create_blockMatrix(mEntries2)","user":"anonymous","dateUpdated":"2018-08-18T01:31:04+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false},"editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"blockM1: org.apache.spark.mllib.linalg.distributed.BlockMatrix = org.apache.spark.mllib.linalg.distributed.BlockMatrix@6108257f\nblockM2: org.apache.spark.mllib.linalg.distributed.BlockMatrix = org.apache.spark.mllib.linalg.distributed.BlockMatrix@2afec51e\n"}]},"apps":[],"jobName":"paragraph_1534545085460_-1264322474","id":"20180816-053544_199438445","dateCreated":"2018-08-17T22:31:25+0000","dateStarted":"2018-08-18T01:17:52+0000","dateFinished":"2018-08-18T01:17:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:208"},{"text":"%md\nFinally, to perform the optimized multiplication use the multiply method of the `BlockMatrix`. As an exercise multiply `blockM1` against `blockM2`. Store the result in a variable.\n\n","dateUpdated":"2018-08-18T01:31:04+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":false,"results":{},"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Finally, to perform the optimized multiplication use the multiply method of the <code>BlockMatrix</code>. As an exercise multiply <code>blockM1</code> against <code>blockM2</code>. Store the result in a variable.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1534545085460_-1264322474","id":"20180816-053542_76914317","dateCreated":"2018-08-17T22:31:25+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:209"},{"text":"val result = blockM1.multiply(blockM2)\nresult.toLocalMatrix()","user":"anonymous","dateUpdated":"2018-08-18T01:31:04+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"result: org.apache.spark.mllib.linalg.distributed.BlockMatrix = org.apache.spark.mllib.linalg.distributed.BlockMatrix@2fef9f16\nres57: org.apache.spark.mllib.linalg.Matrix =\n253.43065250152858  243.28105920476779  254.39209252972825  ... (1024 total)\n259.34540953940063  249.47041941461381  257.2529840944396   ...\n258.2743426910897   257.8530350003406   259.4343781443863   ...\n252.16138529612311  245.58011575739602  248.6006413702766   ...\n262.53082357657877  250.03741360433614  260.5018077284704   ...\n258.8377121620056   254.83122971256856  260.1780194808544   ...\n265.9163630311663   250.9183821229125   263.49897595507247  ...\n260.3604118426498   252.50799646868734  260.6809370470172   ...\n256.4003147135      247.37901068937538  251.63819873736927  ...\n264.9529858200361   253.30959708643562  258.26588577642264  ...\n246.5651090167154   241.34792046680428  248.98042928788698  ...\n259.0167042693879   251.3644007308..."}]},"apps":[],"jobName":"paragraph_1534545085461_-1264707223","id":"20180816-054044_1121238451","dateCreated":"2018-08-17T22:31:25+0000","dateStarted":"2018-08-18T01:21:11+0000","dateFinished":"2018-08-18T01:21:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:210"},{"text":"%md\nOnce the computation is over, save the results to your output bucket. To do so, you will need to transform the `result` object into a `CoordinateMatrix`. Then point to its `entries` attribute, using the `saveAsTextFile` method. Use the following GCS URI template:\nMake sure to replace `<user-name>` with your personal username ID.\n","dateUpdated":"2018-08-18T01:31:53+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Once the computation is over, save the results to your output bucket. To do so, you will need to transform the <code>result</code> object into a <code>CoordinateMatrix</code>. Then point to its <code>entries</code> attribute, using the <code>saveAsTextFile</code> method. Use the following GCS URI template:<br/>Make sure to replace <code>&lt;user-name&gt;</code> with your personal username ID.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1534545085461_-1264707223","id":"20180816-051040_524958004","dateCreated":"2018-08-17T22:31:25+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:211"},{"text":"val bucket = \"gs://de-training-output-fherdelpino/matrix-multiplication-assignment-result\"\nresult.toCoordinateMatrix().entries.saveAsTextFile(bucket)","dateUpdated":"2018-08-18T01:32:05+0000","config":{"tableHide":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.lang.IllegalArgumentException: Invalid bucket name (de-training-output-<user-name>) or object name ()\n  at com.google.cloud.hadoop.gcsio.LegacyPathCodec.getPath(LegacyPathCodec.java:99)\n  at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem.configureBuckets(GoogleHadoopFileSystem.java:75)\n  at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:2011)\n  at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1102)\n  at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1065)\n  at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2812)\n  at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:100)\n  at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2849)\n  at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2831)\n  at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)\n  at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)\n  at org.apache.spark.internal.io.SparkHadoopWriterUtils$.createPathFromString(SparkHadoopWriterUtils.scala:55)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1069)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:961)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:961)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:961)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:960)\n  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1489)\n  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1468)\n  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1468)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1468)\n  ... 46 elided\nCaused by: java.net.URISyntaxException: Illegal character in authority at index 5: gs://de-training-output-<user-name>/\n  at java.net.URI$Parser.fail(URI.java:2848)\n  at java.net.URI$Parser.parseAuthority(URI.java:3186)\n  at java.net.URI$Parser.parseHierarchical(URI.java:3097)\n  at java.net.URI$Parser.parse(URI.java:3053)\n  at java.net.URI.<init>(URI.java:588)\n  at com.google.cloud.hadoop.gcsio.LegacyPathCodec.getPath(LegacyPathCodec.java:93)\n  ... 78 more\n"}]},"apps":[],"jobName":"paragraph_1534545085461_-1264707223","id":"20180816-055550_1811176051","dateCreated":"2018-08-17T22:31:25+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:212"},{"text":"%md\n**Note:** we are converting the `BlockMatrix` back to `CoordinateMatrix` for writing purposes.","dateUpdated":"2018-08-18T01:31:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong>Note:</strong> we are converting the <code>BlockMatrix</code> back to <code>CoordinateMatrix</code> for writing purposes.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1534545085463_-1263937725","id":"20180816-055549_596649276","dateCreated":"2018-08-17T22:31:25+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:213"},{"text":"%md\n## Ensure you convert this notebook into a JAR to submit directly to your cluster for the output to be valid","dateUpdated":"2018-08-18T01:31:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Ensure you convert this notebook into a JAR to submit directly to your cluster for the output to be valid</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1534545085463_-1263937725","id":"20180816-061612_1285669371","dateCreated":"2018-08-17T22:31:25+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:214"},{"text":"%md\n","dateUpdated":"2018-08-18T01:31:04+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1534545085464_-1265861470","id":"20180816-204005_1297364994","dateCreated":"2018-08-17T22:31:25+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:215"}],"name":"Matrix-Multiplication-Scala","id":"2DPU79PJ2","angularObjects":{"2DPCN9GEK:shared_process":[],"2DPMUM1MZ:shared_process":[],"2DQ4KSB5A:shared_process":[],"2DQKFUJ2S:shared_process":[],"2DM9MWDXS:shared_process":[],"2DQVUYZCR:shared_process":[],"2DNWNF1WW:shared_process":[],"2DQ3DG8SW:shared_process":[],"2DQBDSTJM:shared_process":[],"2DNK4KEA9:shared_process":[],"2DM7JTMMG:shared_process":[],"2DQD5P7ZJ:shared_process":[],"2DPYVSA4H:shared_process":[],"2DNANZ78K:shared_process":[],"2DMTDPCE2:shared_process":[],"2DQRRUZW3:shared_process":[],"2DNN66569:shared_process":[],"2DP2CKC1H:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}